## 프로젝트 기획 및 의도
- 이 프로젝트의 핵심 의도는 사후 대응이 아닌 실시간 사전 예방에 있습니다. 
- 기존의 시스템이 로그를 쌓아두고 문제가 발생했을 때 분석하는 방식이었다면, 
- 이 시스템은 실시간으로 발생하는 로그인 데이터를 분석하여 공격 시도를 즉시 탐지하고 신속하게 대응하는 것을 목표로 합니다.

## 이를 통해 다음과 같은 가치를 창출할 수 있습니다.

- 보안 강화: 무차별 대입 공격(Brute Force), 계정 도용(Account Takeover) 등 잠재적인 보안 위협으로부터 사용자의 계정과 시스템을 보호합니다.

- 안정적인 서비스 운영: 비정상적인 트래픽을 조기에 차단하여 서비스의 안정성을 높이고, 사용자에게 신뢰를 줍니다.

- 최신 기술 스택 역량 확보: 이벤트 기반 아키텍처의 핵심인 Kafka와 대용량 데이터 처리에 강점을 가진 
- Spring Boot, Python을 연계하여 현대적인 백엔드 시스템 설계 및 구축 경험을 쌓습니다.


## 프로젝트 개요
- 이 시스템은 사용자의 로그인 실패 이벤트를 실시간으로 수집, 분석하여 정의된 이상 패턴이 감지되면 이를 데이터베이스에 기록하고 
- 관리자에게 알리는 이벤트 기반(Event-driven) 시스템입니다.

## 전체 동작 흐름

- 1. 이벤트 발생 (Spring Boot): 사용자가 로그인 시도 시, 성공/실패 여부를 포함한 로그(계정 ID, 접속 IP, 시간 등)가 생성됩니다.

- 2. 로그 수집 (Filebeat): Filebeat가 실시간으로 로그 파일의 변경을 감지하여 해당 로그 데이터를 Kafka로 전송합니다.

- 3. 데이터 스트리밍 (Kafka): 로그인 실패 로그는 Kafka의 특정 토픽(e.g., login-failure-log)에 적재되어 
- 실시간 처리가 가능한 데이터 파이프라인을 형성합니다.

- 4. 이상 탐지 (Python): Python으로 작성된 분석 애플리케이션(Consumer)이 Kafka 토픽을 구독(Subscribe)하며 실시간으로 들어오는 로그를 분석합니다.
-사전에 정의된 규칙 (예: 5분 내 5회 이상 실패)에 부합하는지 검사합니다.
- 규칙 위반 시, '이상 로그'로 판단합니다.

- 5. 결과 저장 및 알림 (MySQL & Spring Boot):
- 탐지된 이상 로그는 MySQL 데이터베이스에 저장됩니다.
- 동시에 관리자에게 알림(예: 슬랙, 이메일)을 보내거나, Spring Boot API를 호출하여 후속 조치(계정 임시 잠금 등)를 트리거할 수 있습니다.

## 프로젝트의 필요성
- 1. 지능화되는 보안 위협: 자동화된 봇을 이용한 크리덴셜 스터핑(Credential Stuffing), 무차별 대입 공격 등은 더 이상 특별한 공격이 아닌,
- 모든 서비스가 기본적으로 방어해야 할 대상이 되었습니다.

- 2. 신속한 대응의 중요성: 계정 탈취는 단 몇 분 만에 발생할 수 있으며, 이로 인한 피해(개인정보 유출, 금전적 손실)는 막대합니다. 
- 실시간 탐지 및 대응은 피해를 최소화하는 가장 효과적인 방법입니다.

- 3. 분산 아키텍처 경험: 단일 서버(Monolithic)에서 모든 것을 처리하는 것이 아니라, 각 컴포넌트(API서버, 데이터 파이프라인, 분석 시스템)가 \
- 역할을 분담하는 MSA(마이크로서비스 아키텍처)의 기본 개념을 적용하고 경험해 볼 수 있습니다. 이는 확장성과 유지보수성을 높이는 현대적인 개발 패러다임입니다.

## 프로젝트 진행 시 신경써야 할 부분
성공적인 프로젝트를 위해 다음 사항들을 깊이 고민해야 합니다.

1. 데이터 포맷 표준화
   가장 먼저 해야 할 일입니다. Spring Boot에서 생성하는 로그를 일관된 JSON 형식으로 만드는 것이 중요합니다. 
   파싱이 용이해야 Python 분석 로직이 단순해지고 성능이 향상됩니다.

필수 포함 정보: event_timestamp (이벤트 발생 시간), user_id, source_ip (접속 IP), login_success (로그인 성공 여부), 
              user_agent 등 분석에 필요한 데이터를 명확히 정의해야 합니다.

2. 분석 로직의 상태 관리 (Stateful Streaming)
   '5분 이내 5회'와 같은 규칙을 분석하려면 Python 분석기(Consumer)는 이전 데이터를 기억하고 있어야 합니다. 이를 '상태(State) 관리'라고 합니다.

- 초기 단계: Python의 딕셔너리 같은 인메모리(In-memory) 구조를 활용하여 { "user123": [실패시간1, 실패시간2, ...], 
- "123.123.123.123": [계정A, 계정B, ...] } 와 같은 형태로 데이터를 관리할 수 있습니다.

- 심화 단계: 시스템 확장이나 안정성을 고려한다면 Redis 같은 외부 키-값 저장소를 활용하여 상태를 관리하는 것이 좋습니다. 
- 이렇게 하면 Python 분석 서버가 여러 대가 되거나, 장애로 재시작되어도 상태를 잃지 않습니다.

3. 성능 및 확장성
   Kafka 파티션: 로그인 트래픽이 많은 대규모 서비스를 가정한다면, Kafka 토픽의 파티션을 여러 개로 나누고 Python Consumer도 
   여러 개를 띄워 병렬 처리(Consumer Group)하는 구조를 고려해야 합니다.

- 분석 로직의 효율성: Python 코드가 데이터 스트림의 속도를 따라가지 못하면 병목 현상이 발생합니다. 
- 데이터 처리 로직을 최대한 효율적으로 작성해야 합니다.

4. 오탐(False Positive) 관리
   정의한 규칙이 너무 민감하면, 실제로 공격이 아닌데도 이상 로그로 판단하는 '오탐'이 발생할 수 있습니다. 
   (예: 사용자가 정말 비밀번호를 잊어서 여러 번 시도하는 경우)

- 따라서 탐지 규칙의 임계값(예: 5회 실패, 5분)을 상황에 맞게 조절할 수 있도록 설계하는 것이 좋습니다.

- 탐지 시 바로 계정을 영구 잠금하기보다는 임시 잠금 후 본인인증을 통해 해제하는 식의 유연한 정책을 적용하는 것이 바람직합니다.

5. IP 주소 데이터 활용
   '이전 접속 IP와 다른 IP'를 탐지하려면 사용자의 최종 성공 로그인 IP를 별도로 저장하고 있어야 합니다. 
   (주로 User 테이블에 last_login_ip 같은 컬럼으로 관리)

- 단순히 IP가 다른 것만으로 판단하면, 사용자가 모바일 <-> PC 환경을 오갈 때마다 탐지될 수 있습니다. 
- 따라서 **IP 주소의 국가/지역 정보(GeoIP)**를 활용하여 '국내가 아닌 해외 IP에서 갑자기 접속'과 같이 규칙을 더 정교하게 만들 필요가 있습니다. 
- 이는 외부 API나 GeoIP 데이터베이스를 연동하여 구현할 수 있습니다.






