# Kubernetes í™˜ê²½ì—ì„œì˜ ìê°€ ë³µêµ¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ
(.with_claude-code)
## ëª©ì°¨
1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™](#ì•„í‚¤í…ì²˜-ì„¤ê³„-ì›ì¹™)
3. [ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ êµ¬ì„±](#ëª¨ë‹ˆí„°ë§-ìŠ¤íƒ-êµ¬ì„±)
4. [ìê°€ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜](#ìê°€-ë³µêµ¬-ë©”ì»¤ë‹ˆì¦˜)
5. [ì¹´ì˜¤ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ í†µí•©](#ì¹´ì˜¤ìŠ¤-ì—”ì§€ë‹ˆì–´ë§-í†µí•©)
6. [Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘](#prometheus-ë©”íŠ¸ë¦­-ìˆ˜ì§‘)
7. [Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±](#grafana-ëŒ€ì‹œë³´ë“œ-êµ¬ì„±)
8. [ìë™ ì•Œë¦¼ ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜](#ìë™-ì•Œë¦¼-ë°-ì—ìŠ¤ì»¬ë ˆì´ì…˜)
9. [ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ](#ì‹¤ì œ-êµ¬í˜„-ì˜ˆì‹œ)
10. [ì„±ëŠ¥ ìµœì í™” ë° ìš´ì˜](#ì„±ëŠ¥-ìµœì í™”-ë°-ìš´ì˜)

## ì‹œìŠ¤í…œ ê°œìš”

### 1. ëª©í‘œ ë° ë¹„ì „
```yaml
ëª©í‘œ:
  - 99.9% ì´ìƒì˜ ì‹œìŠ¤í…œ ê°€ìš©ì„± ë‹¬ì„±
  - í‰ê·  ë³µêµ¬ ì‹œê°„(MTTR) 3ë¶„ ì´ë‚´ ë‹¬ì„±
  - ì¸ì  ê°œì… ì—†ëŠ” ìë™ ì¥ì•  ëŒ€ì‘
  - ì˜ˆì¸¡ì  ì¥ì•  ê°ì§€ ë° ì‚¬ì „ ëŒ€ì‘

í•µì‹¬ ê¸°ëŠ¥:
  - ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§
  - AI ê¸°ë°˜ ì´ìƒ íƒì§€ ë° ì˜ˆì¸¡ ë¶„ì„
  - ìë™ ìŠ¤ì¼€ì¼ë§ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
  - ì¹´ì˜¤ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë°˜ ì‹œìŠ¤í…œ ê²€ì¦
```

### 2. ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ
```mermaid
graph TB
    A[Kubernetes Cluster] --> B[Prometheus]
    A --> C[Grafana]
    A --> D[Alertmanager]
    A --> E[LitmusChaos]
    A --> F[Robusta]
    
    B --> G[ë©”íŠ¸ë¦­ ìˆ˜ì§‘]
    C --> H[ì‹œê°í™” ëŒ€ì‹œë³´ë“œ]
    D --> I[ì•Œë¦¼ ê´€ë¦¬]
    E --> J[ì¹´ì˜¤ìŠ¤ í…ŒìŠ¤íŠ¸]
    F --> K[ìë™ ë³µêµ¬]
    
    G --> L[OpenTelemetry]
    H --> M[Golden Signals]
    I --> N[Slack/PagerDuty]
    J --> O[ì¥ì•  ì‹œë®¬ë ˆì´ì…˜]
    K --> P[AI ê¸°ë°˜ ë¶„ì„]
```

## ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™

### 1. Google SRE ì›ì¹™ ì ìš©
```yaml
# SRE ì›ì¹™
Error Budget: "ì‹œìŠ¤í…œ ê°€ìš©ì„± ëª©í‘œë¥¼ ì„¤ì •í•˜ê³ , ì—ëŸ¬ ì˜ˆì‚° ë‚´ì—ì„œ ìš´ì˜"
Service Level Objectives (SLOs):
  - ê°€ìš©ì„±: 99.9% (ì›” 43ë¶„ ë‹¤ìš´íƒ€ì„ í—ˆìš©)
  - ì‘ë‹µì‹œê°„: 95% ìš”ì²­ì´ 200ms ì´ë‚´
  - ì²˜ë¦¬ëŸ‰: ì´ˆë‹¹ 10,000 ìš”ì²­ ì²˜ë¦¬
  - ì—ëŸ¬ìœ¨: 0.1% ì´í•˜

Toil ìµœì†Œí™”: "ë°˜ë³µì ì´ê³  ìˆ˜ë™ì ì¸ ì‘ì—…ì„ ìë™í™”"
ëª¨ë‹ˆí„°ë§ ì² í•™: "ì¦ìƒì— ëŒ€í•œ ëª¨ë‹ˆí„°ë§, ì›ì¸ì— ëŒ€í•œ ë¡œê¹…"
```

### 2. ì˜µì €ë²„ë¹Œë¦¬í‹° Three Pillars
```yaml
Metrics:
  - Golden Signals (Latency, Traffic, Errors, Saturation)
  - Infrastructure Metrics
  - Business Metrics
  - Custom Application Metrics

Logs:
  - Structured Logging (JSON)
  - Distributed Tracing Integration
  - Contextual Information
  - Error Stack Traces

Traces:
  - Request Flow Tracking
  - Service Dependency Mapping
  - Performance Bottleneck Identification
  - Cross-Service Communication Analysis
```

## ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ êµ¬ì„±

### 1. Prometheus ìŠ¤íƒ ë°°í¬
```yaml
# prometheus-values.yaml
prometheus:
  prometheusSpec:
    retention: 30d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: fast-ssd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi
    
    additionalScrapeConfigs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)

grafana:
  enabled: true
  adminPassword: secure-password
  datasources:
    datasources.yaml:
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server:80
          isDefault: true
        - name: Loki
          type: loki
          url: http://loki:3100

alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
    receivers:
      - name: 'web.hook'
        slack_configs:
          - api_url: 'YOUR_SLACK_WEBHOOK_URL'
            channel: '#alerts'
            title: 'Kubernetes Alert'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

### 2. í—¬ë¦„ ì°¨íŠ¸ ë°°í¬
```bash
# Prometheus Operator ì„¤ì¹˜
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

kubectl create namespace monitoring
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values prometheus-values.yaml

# Grafana ì ‘ì† ì •ë³´ í™•ì¸
kubectl get secret --namespace monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
```

### 3. OpenTelemetry êµ¬ì„±
```yaml
# otel-collector-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: monitoring
data:
  config.yaml: |
    receivers:
      prometheus:
        config:
          scrape_configs:
            - job_name: 'k8s-pods'
              kubernetes_sd_configs:
                - role: pod
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
      
    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      memory_limiter:
        limit_mib: 512
    
    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"
      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true
    
    service:
      pipelines:
        metrics:
          receivers: [prometheus]
          processors: [memory_limiter, batch]
          exporters: [prometheus]
        traces:
          receivers: [jaeger]
          processors: [memory_limiter, batch]
          exporters: [jaeger]
```

## ìê°€ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

### 1. Kubernetes ë„¤ì´í‹°ë¸Œ ìê°€ ë³µêµ¬
```yaml
# deployment-with-health-checks.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: sample-app:latest
        ports:
        - containerPort: 8080
        
        # ìƒì¡´ì„± ê²€ì‚¬ (Liveness Probe)
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # ì¤€ë¹„ì„± ê²€ì‚¬ (Readiness Probe)
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        # ì‹œì‘ ê²€ì‚¬ (Startup Probe)
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
        
        # ë¦¬ì†ŒìŠ¤ ì œí•œ
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        
        # í™˜ê²½ ë³€ìˆ˜
        env:
        - name: JAVA_OPTS
          value: "-Xmx400m -XX:+UseG1GC"
```

### 2. Horizontal Pod Autoscaler (HPA)
```yaml
# hpa-config.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: sample-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sample-app
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
```

### 3. Vertical Pod Autoscaler (VPA)
```yaml
# vpa-config.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: sample-app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sample-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: app
      maxAllowed:
        cpu: "2"
        memory: "2Gi"
      minAllowed:
        cpu: "100m"
        memory: "128Mi"
      controlledResources: ["cpu", "memory"]
```

### 4. Pod Disruption Budget (PDB)
```yaml
# pdb-config.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: sample-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: sample-app
```

## ì¹´ì˜¤ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ í†µí•©

### 1. LitmusChaos ì„¤ì¹˜ ë° êµ¬ì„±
```bash
# LitmusChaos ì„¤ì¹˜
kubectl create namespace litmus
kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v3.0.0.yaml

# ChaosCenter ì ‘ì† (ì„ íƒì‚¬í•­)
kubectl port-forward -n litmus svc/chaos-litmus-frontend-service 9091:9091
```

### 2. ì¹´ì˜¤ìŠ¤ ì‹¤í—˜ ì •ì˜
```yaml
# pod-delete-chaos.yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: pod-delete-chaos
  namespace: default
spec:
  appinfo:
    appns: default
    applabel: "app=sample-app"
    appkind: deployment
  engineState: active
  chaosServiceAccount: litmus-admin
  experiments:
  - name: pod-delete
    spec:
      components:
        env:
        - name: TOTAL_CHAOS_DURATION
          value: '300'
        - name: CHAOS_INTERVAL
          value: '10'
        - name: FORCE
          value: 'false'
        - name: PODS_AFFECTED_PERC
          value: '30'
      probe:
      - name: "application-health-check"
        type: "httpProbe"
        httpProbe:
          url: "http://sample-app-service:8080/health"
          insecureSkipTLS: false
          method:
            get:
              criteria: ==
              responseCode: "200"
        mode: "Continuous"
        runProperties:
          probeTimeout: 5
          interval: 2
          retry: 1
```

### 3. ë„¤íŠ¸ì›Œí¬ ì¹´ì˜¤ìŠ¤ ì‹¤í—˜
```yaml
# network-latency-chaos.yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: network-latency-chaos
  namespace: default
spec:
  appinfo:
    appns: default
    applabel: "app=sample-app"
    appkind: deployment
  engineState: active
  chaosServiceAccount: litmus-admin
  experiments:
  - name: pod-network-latency
    spec:
      components:
        env:
        - name: TOTAL_CHAOS_DURATION
          value: '600'
        - name: NETWORK_LATENCY
          value: '2000'
        - name: CONTAINER_RUNTIME
          value: 'containerd'
        - name: SOCKET_PATH
          value: '/run/containerd/containerd.sock'
        - name: PODS_AFFECTED_PERC
          value: '50'
      probe:
      - name: "response-time-check"
        type: "httpProbe"
        httpProbe:
          url: "http://sample-app-service:8080/api/health"
          insecureSkipTLS: false
          responseTimeout: 5000
          method:
            get:
              criteria: <
              responseCode: "3000"
        mode: "Edge"
        runProperties:
          probeTimeout: 10
          interval: 5
          retry: 3
```

### 4. ë¦¬ì†ŒìŠ¤ ì¹´ì˜¤ìŠ¤ ì‹¤í—˜
```yaml
# cpu-stress-chaos.yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: cpu-stress-chaos
  namespace: default
spec:
  appinfo:
    appns: default
    applabel: "app=sample-app"
    appkind: deployment
  engineState: active
  chaosServiceAccount: litmus-admin
  experiments:
  - name: pod-cpu-hog
    spec:
      components:
        env:
        - name: TOTAL_CHAOS_DURATION
          value: '300'
        - name: CPU_CORES
          value: '2'
        - name: PODS_AFFECTED_PERC
          value: '25'
        - name: CONTAINER_RUNTIME
          value: 'containerd'
        - name: SOCKET_PATH
          value: '/run/containerd/containerd.sock'
```

## Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘

### 1. Golden Signals ë©”íŠ¸ë¦­ ì •ì˜
```yaml
# golden-signals-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: golden-signals-rules
  namespace: monitoring
spec:
  groups:
  - name: golden.signals
    rules:
    
    # Latency (ì§€ì—°ì‹œê°„)
    - alert: HighLatency
      expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) > 0.5
      for: 5m
      labels:
        severity: warning
        signal: latency
      annotations:
        summary: "High latency detected"
        description: "95th percentile latency is {{ $value }}s for {{ $labels.job }}"
    
    # Traffic (íŠ¸ë˜í”½)
    - alert: HighTraffic
      expr: rate(http_requests_total[5m]) > 100
      for: 2m
      labels:
        severity: info
        signal: traffic
      annotations:
        summary: "High traffic detected"
        description: "Request rate is {{ $value }} req/s for {{ $labels.job }}"
    
    # Errors (ì˜¤ë¥˜ìœ¨)
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
      for: 1m
      labels:
        severity: critical
        signal: errors
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}"
    
    # Saturation (í¬í™”ë„)
    - alert: HighCPUSaturation
      expr: 1 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) > 0.9
      for: 3m
      labels:
        severity: warning
        signal: saturation
      annotations:
        summary: "High CPU saturation detected"
        description: "CPU usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"
    
    - alert: HighMemorySaturation
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.9
      for: 3m
      labels:
        severity: warning
        signal: saturation
      annotations:
        summary: "High memory saturation detected"
        description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"
```

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ì˜ˆì‹œ
```java
// Spring Boot + Micrometer ì˜ˆì‹œ
@RestController
public class MetricsController {
    
    private final MeterRegistry meterRegistry;
    private final Counter requestCounter;
    private final Timer requestTimer;
    private final Gauge activeConnections;
    
    public MetricsController(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        this.requestCounter = Counter.builder("http_requests_total")
                .description("Total HTTP requests")
                .register(meterRegistry);
        this.requestTimer = Timer.builder("http_request_duration_seconds")
                .description("HTTP request duration")
                .register(meterRegistry);
        this.activeConnections = Gauge.builder("active_connections")
                .description("Active database connections")
                .register(meterRegistry, this, MetricsController::getActiveConnections);
    }
    
    @GetMapping("/api/users")
    @Timed(name = "get_users", description = "Time taken to get users")
    public List<User> getUsers() {
        return Timer.Sample.start(meterRegistry)
                .stop(requestTimer.tag("endpoint", "/api/users"))
                .recordCallable(() -> {
                    requestCounter.increment(Tags.of("endpoint", "/api/users", "status", "200"));
                    return userService.getAllUsers();
                });
    }
    
    private double getActiveConnections() {
        return connectionPool.getActiveCount();
    }
}
```

### 3. Kubernetes ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­
```yaml
# resource-monitoring-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: resource-monitoring-rules
  namespace: monitoring
spec:
  groups:
  - name: kubernetes.resources
    rules:
    
    # Pod ì¬ì‹œì‘ ëª¨ë‹ˆí„°ë§
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[5m]) * 60 * 5 > 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Pod is crash looping"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
    
    # ë…¸ë“œ ë©”ëª¨ë¦¬ ë¶€ì¡±
    - alert: NodeMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "Node memory pressure"
        description: "Node {{ $labels.node }} is under memory pressure"
    
    # PVC ìŠ¤í† ë¦¬ì§€ ë¶€ì¡±
    - alert: PersistentVolumeUsageHigh
      expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC usage high"
        description: "PVC {{ $labels.persistentvolumeclaim }} usage is {{ $value | humanizePercentage }}"
```

## Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±

### 1. Golden Signals ëŒ€ì‹œë³´ë“œ
```json
{
  "dashboard": {
    "id": null,
    "title": "Golden Signals Dashboard",
    "tags": ["kubernetes", "golden-signals"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate (Traffic)",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (job, instance)",
            "legendFormat": "{{job}} - {{instance}}"
          }
        ],
        "yAxes": [
          {
            "label": "requests/sec",
            "min": 0
          }
        ],
        "alert": {
          "conditions": [
            {
              "query": {"params": ["A", "5m", "now"]},
              "reducer": {"params": [], "type": "last"},
              "evaluator": {"params": [100], "type": "gt"}
            }
          ],
          "executionErrorState": "alerting",
          "for": "5m",
          "frequency": "10s",
          "handler": 1,
          "name": "High Traffic Alert",
          "noDataState": "no_data"
        }
      },
      {
        "id": 2,
        "title": "Response Time (Latency)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le))",
            "legendFormat": "50th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "seconds",
            "min": 0
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "singlestat",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))",
            "legendFormat": "Error Rate"
          }
        ],
        "valueName": "current",
        "format": "percentunit",
        "thresholds": "0.01,0.05",
        "colorBackground": true
      },
      {
        "id": 4,
        "title": "Resource Saturation",
        "type": "graph",
        "targets": [
          {
            "expr": "1 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])))",
            "legendFormat": "CPU Usage - {{instance}}"
          },
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))",
            "legendFormat": "Memory Usage - {{instance}}"
          }
        ],
        "yAxes": [
          {
            "label": "percent",
            "min": 0,
            "max": 1
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
```

### 2. Kubernetes í´ëŸ¬ìŠ¤í„° ëŒ€ì‹œë³´ë“œ
```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Overview",
    "panels": [
      {
        "title": "Pod Status",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(kube_pod_status_phase{phase=\"Running\"})",
            "legendFormat": "Running Pods"
          },
          {
            "expr": "sum(kube_pod_status_phase{phase=\"Pending\"})",
            "legendFormat": "Pending Pods"
          },
          {
            "expr": "sum(kube_pod_status_phase{phase=\"Failed\"})",
            "legendFormat": "Failed Pods"
          }
        ]
      },
      {
        "title": "Node Resource Usage",
        "type": "heatmap",
        "targets": [
          {
            "expr": "sum by (node) (rate(container_cpu_usage_seconds_total[5m]))",
            "legendFormat": "{{node}}"
          }
        ]
      },
      {
        "title": "Network I/O",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(container_network_receive_bytes_total[5m])) by (pod)",
            "legendFormat": "Received - {{pod}}"
          },
          {
            "expr": "sum(rate(container_network_transmit_bytes_total[5m])) by (pod)",
            "legendFormat": "Transmitted - {{pod}}"
          }
        ]
      }
    ]
  }
}
```

## ìë™ ì•Œë¦¼ ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜

### 1. Alertmanager ê³ ê¸‰ êµ¬ì„±
```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-main
  namespace: monitoring
stringData:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'default-receiver'
      routes:
      - match:
          severity: critical
        receiver: 'critical-receiver'
        group_wait: 5s
        repeat_interval: 30m
        routes:
        - match:
            alertname: 'PodCrashLooping'
          receiver: 'pod-crash-receiver'
          continue: true
      - match:
          severity: warning
        receiver: 'warning-receiver'
        group_wait: 30s
        repeat_interval: 2h
      - match:
          signal: latency
        receiver: 'performance-receiver'
    
    receivers:
    - name: 'default-receiver'
      slack_configs:
      - channel: '#general-alerts'
        title: 'Kubernetes Alert - {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
    
    - name: 'critical-receiver'
      slack_configs:
      - channel: '#critical-alerts'
        title: 'ğŸš¨ CRITICAL ALERT - {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
    
    - name: 'pod-crash-receiver'
      slack_configs:
      - channel: '#infrastructure'
        title: 'ğŸ”„ Pod Restart Alert'
        text: |
          Pod {{ .CommonLabels.pod }} in namespace {{ .CommonLabels.namespace }} is crash looping.
          Please check pod logs and investigate immediately.
          
          kubectl logs -n {{ .CommonLabels.namespace }} {{ .CommonLabels.pod }} --previous
      webhook_configs:
      - url: 'http://robusta-forwarder:80/api/alerts'
        send_resolved: true
    
    - name: 'warning-receiver'
      slack_configs:
      - channel: '#warnings'
        title: 'âš ï¸ Warning - {{ .GroupLabels.alertname }}'
    
    - name: 'performance-receiver'
      slack_configs:
      - channel: '#performance'
        title: 'ğŸ“ˆ Performance Alert - {{ .GroupLabels.alertname }}'
    
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'instance']
    
    - source_match:
        alertname: 'NodeDown'
      target_match_re:
        alertname: '^(NodeCPU|NodeMemory|NodeDisk).*'
      equal: ['instance']
```

### 2. ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì •ì±…
```yaml
# escalation-policy.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: escalation-policy
  namespace: monitoring
data:
  policy.yml: |
    escalation_levels:
      level_1:
        duration: 5m
        channels: ["#alerts"]
        actions: ["slack_notification"]
      
      level_2:
        duration: 15m
        channels: ["#critical-alerts", "#on-call"]
        actions: ["slack_notification", "email_notification"]
        recipients: ["sre-team@company.com"]
      
      level_3:
        duration: 30m
        channels: ["#incident-response"]
        actions: ["pagerduty_alert", "auto_scaling", "failover"]
        pagerduty_service: "kubernetes-production"
      
      level_4:
        duration: 60m
        actions: ["executive_notification", "emergency_procedures"]
        recipients: ["cto@company.com", "ops-manager@company.com"]
    
    alert_routing:
      critical_alerts:
        - "PodCrashLooping"
        - "NodeDown"
        - "HighErrorRate"
        escalation: [level_1, level_2, level_3]
      
      performance_alerts:
        - "HighLatency"
        - "HighCPUSaturation"
        - "HighMemorySaturation"
        escalation: [level_1, level_2]
      
      capacity_alerts:
        - "PersistentVolumeUsageHigh"
        - "HighTraffic"
        escalation: [level_1, level_2]
```

## ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ

### 1. Robusta ìë™ ë³µêµ¬ êµ¬ì„±
```bash
# Robusta ì„¤ì¹˜
helm repo add robusta https://robusta-charts.storage.googleapis.com
helm repo update
helm install robusta robusta/robusta -f robusta-values.yaml --namespace monitoring
```

```yaml
# robusta-values.yaml
globalConfig:
  signing_key: "YOUR_SIGNING_KEY"
  account_id: "YOUR_ACCOUNT_ID"

kubewatch:
  enabled: true

playbookRepos:
  robusta_playbooks:
    url: "https://github.com/robusta-dev/robusta"

sinksConfig:
- slack_sink:
    name: main_slack_sink
    slack_channel: alerts
    api_key: YOUR_SLACK_BOT_TOKEN

customPlaybooks:
- triggers:
  - on_pod_crash_loop:
      restart_threshold: 5
  actions:
  - logs_enricher:
      show_logs_of_previous_runs: true
  - create_pdb_if_missing: {}
  - restart_crashlooping_pod: {}

- triggers:
  - on_prometheus_alert:
      alert_name: "HighLatency"
  actions:
  - pod_restart_enricher: {}
  - scale_deployment:
      replicas: 5
  - run_bash_command:
      bash_command: "kubectl top pods -n $NAMESPACE --sort-by=cpu"

- triggers:
  - on_prometheus_alert:
      alert_name: "HighMemorySaturation"
  actions:
  - resource_recommender: {}
  - vertical_scale_up:
      memory_increase: "256Mi"

- triggers:
  - on_node_pressure:
      node_condition: "MemoryPressure"
  actions:
  - drain_node_gracefully: {}
  - cordon_node: {}
  - alert_on_hpa_reached_limit: {}
```

### 2. ìê°€ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ
```python
# self_healing_controller.py
import os
import time
import logging
from kubernetes import client, config, watch
from prometheus_client.parser import text_string_to_metric_families
import requests

class SelfHealingController:
    def __init__(self):
        config.load_incluster_config()
        self.v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.autoscaling_v2 = client.AutoscalingV2Api()
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def get_pod_metrics(self, namespace, pod_name):
        """Podì˜ ë©”íŠ¸ë¦­ì„ Prometheusì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        query = f'container_memory_usage_bytes{{pod="{pod_name}", namespace="{namespace}"}}'
        response = requests.get(
            f"http://prometheus-server:80/api/v1/query",
            params={'query': query}
        )
        return response.json()
    
    def restart_unhealthy_pod(self, namespace, pod_name):
        """ë¹„ì •ìƒ Pod ì¬ì‹œì‘"""
        try:
            self.v1.delete_namespaced_pod(name=pod_name, namespace=namespace)
            self.logger.info(f"Restarted unhealthy pod: {namespace}/{pod_name}")
        except Exception as e:
            self.logger.error(f"Failed to restart pod {namespace}/{pod_name}: {e}")
    
    def scale_deployment(self, namespace, deployment_name, replicas):
        """Deployment ìŠ¤ì¼€ì¼ë§"""
        try:
            body = {'spec': {'replicas': replicas}}
            self.apps_v1.patch_namespaced_deployment_scale(
                name=deployment_name,
                namespace=namespace,
                body=body
            )
            self.logger.info(f"Scaled deployment {namespace}/{deployment_name} to {replicas} replicas")
        except Exception as e:
            self.logger.error(f"Failed to scale deployment: {e}")
    
    def auto_heal_memory_pressure(self, pod):
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒí™© ìë™ ë³µêµ¬"""
        namespace = pod.metadata.namespace
        pod_name = pod.metadata.name
        
        # Podì˜ Owner Reference í™•ì¸
        if pod.metadata.owner_references:
            owner = pod.metadata.owner_references[0]
            if owner.kind == "ReplicaSet":
                # Deployment ì´ë¦„ ì¶”ì¶œ
                deployment_name = owner.name.rsplit('-', 1)[0]
                
                # í˜„ì¬ ë ˆí”Œë¦¬ì¹´ ìˆ˜ í™•ì¸
                deployment = self.apps_v1.read_namespaced_deployment(
                    name=deployment_name, namespace=namespace
                )
                current_replicas = deployment.spec.replicas
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë©´ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
                if current_replicas < 10:  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
                    self.scale_deployment(namespace, deployment_name, current_replicas + 2)
                else:
                    # ìµœëŒ€ ë ˆí”Œë¦¬ì¹´ì— ë„ë‹¬í–ˆìœ¼ë©´ ë¬¸ì œ Podë§Œ ì¬ì‹œì‘
                    self.restart_unhealthy_pod(namespace, pod_name)
    
    def watch_pods(self):
        """Pod ìƒíƒœ ê°ì‹œ ë° ìë™ ë³µêµ¬"""
        w = watch.Watch()
        for event in w.stream(self.v1.list_pod_for_all_namespaces):
            pod = event['object']
            event_type = event['type']
            
            if event_type == 'MODIFIED':
                self.handle_pod_event(pod)
    
    def handle_pod_event(self, pod):
        """Pod ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        if not pod.status.container_statuses:
            return
        
        for container_status in pod.status.container_statuses:
            # ì¬ì‹œì‘ íšŸìˆ˜ê°€ ë§ì€ ê²½ìš°
            if container_status.restart_count > 5:
                self.logger.warning(f"Pod {pod.metadata.namespace}/{pod.metadata.name} has high restart count: {container_status.restart_count}")
                self.auto_heal_memory_pressure(pod)
            
            # OOMKilled ìƒíƒœ ê°ì§€
            if (container_status.last_state.terminated and 
                container_status.last_state.terminated.reason == 'OOMKilled'):
                self.logger.error(f"Pod {pod.metadata.namespace}/{pod.metadata.name} was OOMKilled")
                self.auto_heal_memory_pressure(pod)
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        self.logger.info("Starting Self-Healing Controller...")
        try:
            self.watch_pods()
        except Exception as e:
            self.logger.error(f"Controller error: {e}")
            time.sleep(5)
            self.run()  # ì¬ì‹œì‘

if __name__ == "__main__":
    controller = SelfHealingController()
    controller.run()
```

### 3. Deployment ì˜ˆì‹œ
```yaml
# self-healing-controller.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: self-healing-controller
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: self-healing-controller
  template:
    metadata:
      labels:
        app: self-healing-controller
    spec:
      serviceAccountName: self-healing-controller
      containers:
      - name: controller
        image: self-healing-controller:latest
        env:
        - name: PROMETHEUS_URL
          value: "http://prometheus-server:80"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: self-healing-controller
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: self-healing-controller
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list", "watch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "deployments/scale"]
  verbs: ["get", "list", "patch"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: self-healing-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: self-healing-controller
subjects:
- kind: ServiceAccount
  name: self-healing-controller
  namespace: monitoring
```

## ì„±ëŠ¥ ìµœì í™” ë° ìš´ì˜

### 1. Prometheus ìµœì í™”
```yaml
# prometheus-optimization.yaml
prometheus:
  prometheusSpec:
    # ë©”ëª¨ë¦¬ ë° CPU ìµœì í™”
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
    
    # ìŠ¤í† ë¦¬ì§€ ìµœì í™”
    retention: "30d"
    retentionSize: "50GB"
    
    # ì¿¼ë¦¬ ìµœì í™”
    query:
      timeout: 2m
      maxConcurrency: 20
      maxSamples: 50000000
    
    # WAL ì••ì¶• ì„¤ì •
    walCompression: true
    
    # ìŠ¤í¬ë˜í•‘ ìµœì í™”
    evaluationInterval: 30s
    scrapeInterval: 30s
    
    # ì›ê²© ìŠ¤í† ë¦¬ì§€ (ì¥ê¸° ë³´ê´€ìš©)
    remoteWrite:
    - url: "http://thanos-receive:19291/api/v1/receive"
      writeRelabelConfigs:
      - sourceLabels: [__name__]
        regex: 'up|kube_.*|container_.*|node_.*'
        action: keep
```

### 2. ëª¨ë‹ˆí„°ë§ ë¹„ìš© ìµœì í™”
```yaml
# cost-optimization-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cost-optimization-rules
  namespace: monitoring
spec:
  groups:
  - name: cost.optimization
    rules:
    
    # ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ê°ì§€
    - alert: OverProvisionedPods
      expr: avg_over_time(rate(container_cpu_usage_seconds_total[5m])[30m:5m]) < 0.1
      for: 1h
      labels:
        severity: info
        cost_optimization: true
      annotations:
        summary: "Pod {{ $labels.pod }} is over-provisioned"
        description: "CPU utilization is only {{ $value | humanizePercentage }}"
        recommendation: "Consider reducing CPU requests"
    
    - alert: UnusedPersistentVolumes
      expr: kube_persistentvolume_status_phase{phase="Available"} == 1
      for: 24h
      labels:
        severity: info
        cost_optimization: true
      annotations:
        summary: "Unused PV detected"
        description: "PV {{ $labels.persistentvolume }} has been available for 24h"
    
    # ë¹„íš¨ìœ¨ì ì¸ ìŠ¤ì¼€ì¼ë§ ê°ì§€
    - alert: IneffectiveHPA
      expr: kube_horizontalpodautoscaler_status_current_replicas == kube_horizontalpodautoscaler_spec_min_replicas
      for: 6h
      labels:
        severity: info
        cost_optimization: true
      annotations:
        summary: "HPA may be ineffective"
        description: "HPA {{ $labels.horizontalpodautoscaler }} has been at min replicas for 6h"
```

### 3. ë°±ì—… ë° ì¬í•´ ë³µêµ¬
```yaml
# backup-strategy.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: prometheus-backup
  namespace: monitoring
spec:
  schedule: "0 2 * * *"  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: prom/prometheus:latest
            command:
            - /bin/sh
            - -c
            - |
              # Prometheus ë°ì´í„° ë°±ì—…
              promtool tsdb create-blocks-from prometheus \
                --mint=$(date -d '7 days ago' +%s)000 \
                --maxt=$(date +%s)000 \
                /prometheus \
                /backup/$(date +%Y%m%d)
              
              # S3ì— ì—…ë¡œë“œ (ì˜ˆì‹œ)
              aws s3 sync /backup/ s3://prometheus-backup-bucket/
            volumeMounts:
            - name: prometheus-data
              mountPath: /prometheus
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: prometheus-data
            persistentVolumeClaim:
              claimName: prometheus-data
          - name: backup-storage
            emptyDir: {}
          restartPolicy: OnFailure

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: grafana-backup
  namespace: monitoring
spec:
  schedule: "30 2 * * *"  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ 30ë¶„
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: grafana/grafana-cli:latest
            command:
            - /bin/sh
            - -c
            - |
              # Grafana ëŒ€ì‹œë³´ë“œ ë°±ì—…
              for dashboard in $(grafana-cli admin get-dashboards | jq -r '.[] | .uid'); do
                grafana-cli admin export-dashboard $dashboard > /backup/dashboard-$dashboard.json
              done
              
              # Git ì €ì¥ì†Œì— ì»¤ë°‹
              cd /backup && git add . && git commit -m "Daily backup $(date)" && git push
            env:
            - name: GRAFANA_URL
              value: "http://prometheus-grafana"
            - name: GRAFANA_API_KEY
              valueFrom:
                secretKeyRef:
                  name: grafana-api-key
                  key: key
            volumeMounts:
            - name: backup-git
              mountPath: /backup
          volumes:
          - name: backup-git
            gitRepo:
              repository: "https://github.com/company/grafana-backups.git"
          restartPolicy: OnFailure
```

### 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° íŠœë‹
```python
# performance_monitor.py
import time
import psutil
import logging
from kubernetes import client, config
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

class PerformanceMonitor:
    def __init__(self):
        config.load_incluster_config()
        self.v1 = client.CoreV1Api()
        
        # Prometheus ë©”íŠ¸ë¦­ ì„¤ì •
        self.registry = CollectorRegistry()
        self.cpu_gauge = Gauge('monitoring_system_cpu_usage', 'CPU usage of monitoring system', 
                              ['component'], registry=self.registry)
        self.memory_gauge = Gauge('monitoring_system_memory_usage', 'Memory usage of monitoring system', 
                                 ['component'], registry=self.registry)
        self.query_duration_gauge = Gauge('prometheus_query_duration_seconds', 
                                         'Duration of Prometheus queries', 
                                         ['query_type'], registry=self.registry)
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def monitor_prometheus_performance(self):
        """Prometheus ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        try:
            # Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            prometheus_pods = self.v1.list_namespaced_pod(
                namespace='monitoring',
                label_selector='app.kubernetes.io/name=prometheus'
            )
            
            for pod in prometheus_pods.items:
                # CPU ì‚¬ìš©ëŸ‰
                cpu_usage = self.get_pod_cpu_usage(pod)
                self.cpu_gauge.labels(component='prometheus').set(cpu_usage)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                memory_usage = self.get_pod_memory_usage(pod)
                self.memory_gauge.labels(component='prometheus').set(memory_usage)
                
                self.logger.info(f"Prometheus pod {pod.metadata.name}: CPU={cpu_usage}%, Memory={memory_usage}%")
                
        except Exception as e:
            self.logger.error(f"Error monitoring Prometheus performance: {e}")
    
    def monitor_grafana_performance(self):
        """Grafana ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        try:
            grafana_pods = self.v1.list_namespaced_pod(
                namespace='monitoring',
                label_selector='app.kubernetes.io/name=grafana'
            )
            
            for pod in grafana_pods.items:
                cpu_usage = self.get_pod_cpu_usage(pod)
                memory_usage = self.get_pod_memory_usage(pod)
                
                self.cpu_gauge.labels(component='grafana').set(cpu_usage)
                self.memory_gauge.labels(component='grafana').set(memory_usage)
                
                self.logger.info(f"Grafana pod {pod.metadata.name}: CPU={cpu_usage}%, Memory={memory_usage}%")
                
        except Exception as e:
            self.logger.error(f"Error monitoring Grafana performance: {e}")
    
    def get_pod_cpu_usage(self, pod):
        """Pod CPU ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” metrics-server API ë˜ëŠ” cAdvisor ì‚¬ìš©
        return psutil.cpu_percent(interval=1)
    
    def get_pod_memory_usage(self, pod):
        """Pod ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” metrics-server API ë˜ëŠ” cAdvisor ì‚¬ìš©
        return psutil.virtual_memory().percent
    
    def optimize_prometheus_config(self):
        """Prometheus ì„¤ì • ìµœì í™”"""
        optimization_suggestions = []
        
        # ìŠ¤í¬ë˜í•‘ ê°„ê²© ìµœì í™” ì œì•ˆ
        high_cardinality_metrics = self.check_high_cardinality_metrics()
        if high_cardinality_metrics:
            optimization_suggestions.append({
                'type': 'scrape_interval',
                'suggestion': 'Increase scrape interval for high cardinality metrics',
                'metrics': high_cardinality_metrics
            })
        
        # ë³´ì¡´ ì •ì±… ìµœì í™”
        storage_usage = self.check_storage_usage()
        if storage_usage > 80:
            optimization_suggestions.append({
                'type': 'retention',
                'suggestion': 'Consider reducing retention period or implementing tiered storage',
                'current_usage': f'{storage_usage}%'
            })
        
        return optimization_suggestions
    
    def check_high_cardinality_metrics(self):
        """ê³  ì¹´ë””ë„ë¦¬í‹° ë©”íŠ¸ë¦­ í™•ì¸"""
        # Prometheus APIë¥¼ í†µí•´ ë©”íŠ¸ë¦­ ì¹´ë””ë„ë¦¬í‹° í™•ì¸
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Prometheus API ì‚¬ìš©
        return ['container_network_receive_bytes_total', 'container_network_transmit_bytes_total']
    
    def check_storage_usage(self):
        """ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ í™•ì¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Kubernetes PVC ì‚¬ìš©ëŸ‰ í™•ì¸
        return 75  # ì˜ˆì‹œ ê°’
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        while True:
            try:
                self.monitor_prometheus_performance()
                self.monitor_grafana_performance()
                
                # Pushgatewayì— ë©”íŠ¸ë¦­ ì „ì†¡
                push_to_gateway('pushgateway:9091', job='performance-monitor', 
                              registry=self.registry)
                
                # ìµœì í™” ì œì•ˆ ìƒì„±
                suggestions = self.optimize_prometheus_config()
                if suggestions:
                    self.logger.info(f"Optimization suggestions: {suggestions}")
                
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
                
            except Exception as e:
                self.logger.error(f"Error in performance monitoring loop: {e}")
                time.sleep(60)

if __name__ == "__main__":
    monitor = PerformanceMonitor()
    monitor.run()
```

## ê²°ë¡  ë° ëª¨ë²” ì‚¬ë¡€

### 1. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸
```yaml
Phase 1 - ê¸°ë°˜ êµ¬ì¶•:
  âœ… Kubernetes í´ëŸ¬ìŠ¤í„° ì„¤ì •
  âœ… Prometheus + Grafana ì„¤ì¹˜
  âœ… ê¸°ë³¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ êµ¬ì„±
  âœ… ì•Œë¦¼ ì±„ë„ ì„¤ì • (Slack, Email)

Phase 2 - ëª¨ë‹ˆí„°ë§ ê³ ë„í™”:
  âœ… Golden Signals êµ¬í˜„
  âœ… ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê°œë°œ
  âœ… ëŒ€ì‹œë³´ë“œ êµ¬ì„±
  âœ… SLI/SLO ì •ì˜

Phase 3 - ìê°€ ë³µêµ¬:
  âœ… HPA/VPA êµ¬ì„±
  âœ… ìê°€ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ ê°œë°œ
  âœ… Robusta í†µí•©
  âœ… ì¹´ì˜¤ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ ë„ì…

Phase 4 - ìµœì í™”:
  âœ… ì„±ëŠ¥ íŠœë‹
  âœ… ë¹„ìš© ìµœì í™”
  âœ… ë°±ì—… ë° ì¬í•´ ë³µêµ¬
  âœ… ì§€ì†ì  ê°œì„ 
```

### 2. ìš´ì˜ ê°€ì´ë“œë¼ì¸
```yaml
ì¼ì¼ ìš´ì˜:
  - ëŒ€ì‹œë³´ë“œ í™•ì¸ (Golden Signals)
  - ì•Œë¦¼ ê²€í†  ë° ëŒ€ì‘
  - ì„±ëŠ¥ ì§€í‘œ ë¶„ì„
  - ì¹´ì˜¤ìŠ¤ ì‹¤í—˜ ê²°ê³¼ ê²€í† 

ì£¼ê°„ ìš´ì˜:
  - ë©”íŠ¸ë¦­ ì •í™•ì„± ê²€ì¦
  - ì•Œë¦¼ ê·œì¹™ ìµœì í™”
  - ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
  - ìš©ëŸ‰ ê³„íš ìˆ˜ë¦½

ì›”ê°„ ìš´ì˜:
  - SLO ë‹¬ì„±ë¥  í‰ê°€
  - ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬ë·°
  - ë¹„ìš© ìµœì í™” ë¶„ì„
  - ì¬í•´ ë³µêµ¬ í…ŒìŠ¤íŠ¸
```

### 3. ì„±ê³µ ì§€í‘œ (KPI)
```yaml
ê°€ìš©ì„± ì§€í‘œ:
  - ì‹œìŠ¤í…œ ê°€ìš©ì„±: 99.9% ì´ìƒ
  - MTTR (í‰ê·  ë³µêµ¬ ì‹œê°„): 3ë¶„ ì´í•˜
  - MTBF (í‰ê·  ì¥ì•  ê°„ê²©): 30ì¼ ì´ìƒ

ì„±ëŠ¥ ì§€í‘œ:
  - ì‘ë‹µ ì‹œê°„: 95%ile < 200ms
  - ì²˜ë¦¬ëŸ‰: 10,000 RPS
  - ì—ëŸ¬ìœ¨: < 0.1%

ìš´ì˜ íš¨ìœ¨ì„±:
  - ìë™ ë³µêµ¬ìœ¨: 90% ì´ìƒ
  - ê±°ì§“ ì•Œë¦¼ìœ¨: < 5%
  - ìš´ì˜ ì—…ë¬´ ìë™í™”ìœ¨: 80% ì´ìƒ
```

ì´ ê°€ì´ë“œë¥¼ í†µí•´ Kubernetes í™˜ê²½ì—ì„œ Netflixì™€ Google SRE ìˆ˜ì¤€ì˜ ìê°€ ë³µêµ¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìœ¼ë©°, ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ 99.9% ì´ìƒì˜ ê°€ìš©ì„±ê³¼ 3ë¶„ ì´ë‚´ì˜ ë³µêµ¬ ì‹œê°„ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.